model options:
	monitor_grad:	False
	dropout_output:	0.5
	n_words:	10000
	start_epoch:	0
	dataset:	text
	patience:	10
	skip_steps2:	-1
	hier_len:	None
	max_epochs:	5000
	dispFreq:	50
	newDumpFreq:	5000000
	self:	None
	hybrid:	False
	clip_c:	-1.0
	dim_proj:	1024
	saveto:	model.npz
	start_iter:	0
	lastHiddenLayer:	None
	noise_std:	0.0
	batch_len_threshold:	None
	valid_batch_size:	16
	corpus:	imdb.pkl
	reload_options:	None
	optimizer:	adadelta
	validFreq:	2000
	dropout_input:	0.8
	warm_LM:	None
	batch_size:	16
	encoder:	lstm
	hierarchical:	False
	reload_model:	ccdd/warmClassifier.npz
	lrate:	1.0
	truncate_grad:	-1
	decay_c:	-1.0
	encoder2:	None
	test_size:	None
	dim_word:	500
	unit_depth:	-1
	maxlen:	None
	skip_steps:	-1
	embedding:	None
	logFile:	log2
	mean_pooling:	False
-----------------------------------------
model options:
	monitor_grad:	False
	dropout_output:	0.5
	n_words:	10000
	start_epoch:	0
	dataset:	text
	patience:	10
	skip_steps2:	-1
	hier_len:	None
	max_epochs:	5000
	dispFreq:	50
	newDumpFreq:	5000000
	self:	None
	hybrid:	False
	clip_c:	-1.0
	dim_proj:	1024
	saveto:	model.npz
	start_iter:	0
	lastHiddenLayer:	None
	noise_std:	0.0
	batch_len_threshold:	None
	valid_batch_size:	16
	corpus:	imdb.pkl
	reload_options:	None
	optimizer:	adadelta
	validFreq:	2000
	dropout_input:	0.8
	warm_LM:	None
	batch_size:	16
	encoder:	lstm
	hierarchical:	False
	reload_model:	ccdd2/warmClassifier.npz
	lrate:	1.0
	truncate_grad:	-1
	decay_c:	-1.0
	encoder2:	None
	test_size:	None
	dim_word:	500
	unit_depth:	-1
	maxlen:	None
	skip_steps:	-1
	embedding:	None
	logFile:	log2
	mean_pooling:	False
-----------------------------------------
model options:
	monitor_grad:	False
	dropout_output:	0.5
	n_words:	10000
	start_epoch:	0
	dataset:	text
	patience:	10
	skip_steps2:	-1
	hier_len:	None
	max_epochs:	5000
	dispFreq:	50
	newDumpFreq:	5000000
	self:	None
	hybrid:	False
	clip_c:	-1.0
	dim_proj:	1024
	saveto:	model.npz
	start_iter:	0
	lastHiddenLayer:	None
	noise_std:	0.0
	batch_len_threshold:	None
	valid_batch_size:	16
	corpus:	imdb.pkl
	reload_options:	None
	optimizer:	adadelta
	validFreq:	2000
	dropout_input:	0.8
	warm_LM:	None
	batch_size:	16
	encoder:	lstm
	hierarchical:	False
	reload_model:	ccdd2/warmClassifier.npz
	lrate:	1.0
	truncate_grad:	-1
	decay_c:	-1.0
	encoder2:	None
	test_size:	None
	dim_word:	500
	unit_depth:	-1
	maxlen:	None
	skip_steps:	-1
	embedding:	None
	logFile:	log2
	mean_pooling:	False
-----------------------------------------
model options:
	monitor_grad:	False
	dropout_output:	0.5
	n_words:	10000
	start_epoch:	0
	dataset:	text
	patience:	10
	skip_steps2:	-1
	hier_len:	None
	max_epochs:	5000
	dispFreq:	50
	newDumpFreq:	5000000
	self:	None
	hybrid:	False
	clip_c:	-1.0
	dim_proj:	1024
	saveto:	model.npz
	start_iter:	0
	lastHiddenLayer:	None
	noise_std:	0.0
	batch_len_threshold:	None
	valid_batch_size:	16
	corpus:	imdb.pkl
	reload_options:	None
	optimizer:	adadelta
	validFreq:	2000
	dropout_input:	0.8
	warm_LM:	None
	batch_size:	16
	encoder:	lstm
	hierarchical:	False
	reload_model:	ccdd2/warmClassifier.npz
	lrate:	1.0
	truncate_grad:	-1
	decay_c:	-1.0
	encoder2:	None
	test_size:	None
	dim_word:	500
	unit_depth:	-1
	maxlen:	None
	skip_steps:	-1
	embedding:	None
	logFile:	log2
	mean_pooling:	False
-----------------------------------------
model options:
	monitor_grad:	False
	dropout_output:	0.5
	n_words:	10000
	start_epoch:	0
	dataset:	text
	patience:	10
	skip_steps2:	-1
	hier_len:	None
	max_epochs:	5000
	dispFreq:	50
	newDumpFreq:	5000000
	self:	None
	hybrid:	False
	clip_c:	-1.0
	dim_proj:	1024
	saveto:	model.npz
	start_iter:	0
	lastHiddenLayer:	None
	noise_std:	0.0
	batch_len_threshold:	None
	valid_batch_size:	16
	corpus:	imdb.pkl
	reload_options:	None
	optimizer:	adadelta
	validFreq:	2000
	dropout_input:	0.8
	warm_LM:	None
	batch_size:	16
	encoder:	lstm
	hierarchical:	False
	reload_model:	ccdd2/warmClassifier.npz
	lrate:	1.0
	truncate_grad:	-1
	decay_c:	-1.0
	encoder2:	None
	test_size:	None
	dim_word:	500
	unit_depth:	-1
	maxlen:	None
	skip_steps:	-1
	embedding:	None
	logFile:	log2
	mean_pooling:	False
-----------------------------------------
model options:
	monitor_grad:	False
	dropout_output:	0.5
	n_words:	10000
	start_epoch:	0
	dataset:	text
	patience:	10
	skip_steps2:	-1
	hier_len:	None
	max_epochs:	5000
	dispFreq:	50
	newDumpFreq:	5000000
	self:	None
	hybrid:	False
	clip_c:	-1.0
	dim_proj:	1024
	saveto:	model.npz
	start_iter:	0
	lastHiddenLayer:	None
	noise_std:	0.0
	batch_len_threshold:	None
	valid_batch_size:	16
	corpus:	imdb.pkl
	reload_options:	None
	optimizer:	adadelta
	validFreq:	2000
	dropout_input:	0.8
	warm_LM:	None
	batch_size:	16
	encoder:	lstm
	hierarchical:	False
	reload_model:	ccdd2/warmClassifier.npz
	lrate:	1.0
	truncate_grad:	-1
	decay_c:	-1.0
	encoder2:	None
	test_size:	None
	dim_word:	500
	unit_depth:	-1
	maxlen:	None
	skip_steps:	-1
	embedding:	None
	logFile:	log2
	mean_pooling:	False
-----------------------------------------
model options:
	monitor_grad:	False
	dropout_output:	0.5
	n_words:	10000
	start_epoch:	0
	dataset:	text
	patience:	10
	skip_steps2:	-1
	hier_len:	None
	max_epochs:	5000
	dispFreq:	50
	newDumpFreq:	5000000
	self:	None
	hybrid:	False
	clip_c:	-1.0
	dim_proj:	1024
	saveto:	model.npz
	start_iter:	0
	lastHiddenLayer:	None
	noise_std:	0.0
	batch_len_threshold:	None
	valid_batch_size:	16
	corpus:	imdb.pkl
	reload_options:	None
	optimizer:	adadelta
	validFreq:	2000
	dropout_input:	0.8
	warm_LM:	None
	batch_size:	16
	encoder:	lstm
	hierarchical:	False
	reload_model:	ccdd2/warmClassifier.npz
	lrate:	1.0
	truncate_grad:	-1
	decay_c:	-1.0
	encoder2:	None
	test_size:	None
	dim_word:	500
	unit_depth:	-1
	maxlen:	None
	skip_steps:	-1
	embedding:	None
	logFile:	log2
	mean_pooling:	False
-----------------------------------------
